{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Trey Tuscai and Gordon Doore\n",
    "\n",
    "Spring 2025\n",
    "\n",
    "CS 444: Deep Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.style.use(['seaborn-v0_8-colorblind', 'seaborn-v0_8-darkgrid'])\n",
    "plt.rcParams.update({'font.size': 20})\n",
    "\n",
    "np.set_printoptions(suppress=True, precision=4)\n",
    "\n",
    "# Automatically reload your external source code\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- # Project 3 | Word Embeddings and Sentiment Analysis -->\n",
    "# Project 3 | Word Embeddings\n",
    "\n",
    "The goal of this project is to gain insight about the representations learned by neural networks — how do they encode the training data internally? We will transition to training neural network on text data — specifically on tens of thousands of real Amazon reviews of fashion products (e.g. clothes, shoes, etc.). \n",
    "\n",
    "<!-- Next week, you will train a CNN to predict whether the reviews are positive or negative, a task in the field natural language processing (NLP) called **sentiment analysis**. -->\n",
    "\n",
    "<!-- #### Week 1: Word Embeddings of Amazon Fashion Reviews -->\n",
    "\n",
    "This notebook focuses on building the Amazon Fashion Reviews text preprocessing pipeline as well as analyzing and visualizing how a neural network learns to encode words from the reviews. You will train a **Continuous Bag of Words (CBOW)** (word2vec) neural network commonly used in the field of natural language processing (NLP) on text from IMDb user movie reviews. The network attempts to predict a **target word** (a missing word from a passage of text) from the surrounding **context words** (the words surrounding the target word in a sentence). After implementing and training the network, you will extract the weights to obtain $H$ dimensional **word embedding** vectors for English words that appeared in the Amazon reviews to analyze and visualize. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 1: Preprocessing Amazon Fashion Reviews"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1a. Download and inspect first several few Amazon reviews\n",
    "\n",
    "Download the Amazon Fashion dataset (`Amazon_Fashion.jsonl`) from the project website and store it in the `data` subfolder of your working directory. These Amazon reviews were curated by [the McAuley lab in 2023](https://huggingface.co/datasets/McAuley-Lab/Amazon-Reviews-2023).\n",
    "\n",
    "In the cell below, use the provided `load_reviews_and_ratings` function in `amazon_reviews.py` to load in the 1st five reviews and ratings. Print these first five reviews and clearly indicate the star rating for each review."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from amazon_reviews import load_reviews_and_ratings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Review 1:\n",
      "Rating: 5.0 stars\n",
      "Text: I think this locket is really pretty. The inside back is a solid silver depression and the front is a dome that is not solid (knotted). You could use it to store a small photo, lock of hair, etc but I use it when I need to carry medication with me. Closes securely. High quality & very pretty.\n",
      "\n",
      "Review 2:\n",
      "Rating: 5.0 stars\n",
      "Text: Great\n",
      "\n",
      "Review 3:\n",
      "Rating: 2.0 stars\n",
      "Text: One of the stones fell out within the first 2 weeks of wearing it. Stones smaller than expected.\n",
      "\n",
      "Review 4:\n",
      "Rating: 1.0 stars\n",
      "Text: Crappy socks. Money wasted. Bought to wear with my tieks. Don’t stay on feet well.\n",
      "\n",
      "Review 5:\n",
      "Rating: 5.0 stars\n",
      "Text: I LOVE these glasses!  They fit perfectly over my regular, rectangular glasses that I always have to wear in order to see.  I really appreciate having these pretty and stylish and sturdy sunglasses to wear over my glasses.  I'll buy these again and again whenever I need a new pair, which hopefully won't be too soon.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "reviews, ratings = load_reviews_and_ratings(N_reviews=5)\n",
    "\n",
    "for i, (review, rating) in enumerate(zip(reviews, ratings)):\n",
    "    print(f\"Review {i+1}:\")\n",
    "    print(f\"Rating: {rating:.1f} stars\")\n",
    "    print(f\"Text: {review}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1b. Make the corpus\n",
    "\n",
    "We will use a \"flat\"/1D list of sentences across the reviews as our **corpus**. That is, we REMOVE information about which review a sentence came from in the corpus. However, we will eventually need this information in order to determine whether a review is positive/negative so we maintain a separate list that indicates to which review a given sentence belongs. For example: Sent 0 belongs to review 0, Sent 1 belongs to review 0, Sent 2 belongs to review 1, etc.\n",
    "\n",
    "To make sure we have enough context words in sentences, we prune sentences that are too short. We also prune sentences that are too long.\n",
    "\n",
    "Implement `make_corpus` in `amazon_reviews.py` to create the corpus of sentences and review ID associations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from amazon_reviews import make_corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of sentences in the corpus (after pruning short sentences/identifying words): 185.\n",
      "There should be 185 sentences\n",
      "The first few sent ratings are: [5. 5. 5. 5. 5. 2. 2. 1. 1. 1.] and should be [5. 5. 5. 5. 5. 2. 2. 1. 1. 1.]\n",
      "The last few sent ratings are: [2. 2. 2. 2. 2. 2. 2. 2. 2. 2.] and should be [2. 2. 2. 2. 2. 2. 2. 2. 2. 2.]\n",
      "The first few review_ids are: [0 0 0 0 0 2 2 3 3 3] and should be [0 0 0 0 0 2 2 3 3 3]\n",
      "The last few review_ids are: [49 49 49 49 49 49 49 49 49 49] and should be [49 49 49 49 49 49 49 49 49 49]\n"
     ]
    }
   ],
   "source": [
    "corpus, sentence_ratings, review_ids = make_corpus(N_reviews=50)\n",
    "print(f'Number of sentences in the corpus (after pruning short sentences/identifying words): {len(corpus)}.')\n",
    "print('There should be 185 sentences')\n",
    "assert len(corpus) == len(sentence_ratings)\n",
    "assert len(corpus) == len(review_ids)\n",
    "print(f'The first few sent ratings are: {sentence_ratings[:10]} and should be [5. 5. 5. 5. 5. 2. 2. 1. 1. 1.]')\n",
    "print(f'The last few sent ratings are: {sentence_ratings[-10:]} and should be [2. 2. 2. 2. 2. 2. 2. 2. 2. 2.]')\n",
    "print(f'The first few review_ids are: {review_ids[:10]} and should be [0 0 0 0 0 2 2 3 3 3]')\n",
    "print(f'The last few review_ids are: {review_ids[-10:]} and should be [49 49 49 49 49 49 49 49 49 49]')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1c. Make the vocabulary\n",
    "\n",
    "We will be training a **word-level model**, so our tokens will be single words. Implement `find_unique_words` to form the **vocabulary**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from amazon_reviews import find_unique_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "unique words:\n",
      " ['I', 'love', 'CS444', 'deep', 'learning', 'CS', 'Colby']\n",
      "unique words should be:\n",
      " ['I', 'love', 'CS444', 'deep', 'learning', 'CS', 'Colby']\n",
      "Number of unique words in the 50 reviews: 677 and should be 677.\n"
     ]
    }
   ],
   "source": [
    "test_text = [['I', 'love', 'CS444'],\n",
    "             ['I', 'love', 'deep', 'learning'],\n",
    "             ['I', 'love', 'CS'],\n",
    "             ['I', 'love', 'Colby']]\n",
    "unique = find_unique_words(test_text)\n",
    "print('unique words:\\n', unique)\n",
    "print(\"unique words should be:\\n ['I', 'love', 'CS444', 'deep', 'learning', 'CS', 'Colby']\")\n",
    "\n",
    "unique_words_corpus = find_unique_words(corpus)\n",
    "print(f'Number of unique words in the 50 reviews: {len(unique_words_corpus)} and should be 677.')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1d. Mapping words to indices and back\n",
    "\n",
    "While our corpus is currently made of word strings we clearly cannot plug these in as inputs to a neural network! We need to convert these word features to numbers first. For the specific CBOW word2vec neural network we are implementing, we *could plug in each word represented as a one-hot vector. To determine the location of the 1 in each vector, we use the word's position in the vocabulary. \n",
    "\n",
    "Implement and test `make_word2ind_mapping` to convert a string word to its position index in the vocab and `make_ind2word_mapping` to perform the reverse — looking up a string word based on its int index.\n",
    "\n",
    "*For efficiency, we won't *actually* plug in one-hot vectors into CBOW. We'll use the int indices instead."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from amazon_reviews import make_word2ind_mapping, make_ind2word_mapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For the test text, your word2ind mapping is:\n",
      "{'I': 0, 'love': 1, 'CS444': 2, 'deep': 3, 'learning': 4, 'CS': 5, 'Colby': 6}\n",
      "and it should be\n",
      "{'I': 0, 'love': 1, 'CS444': 2, 'deep': 3, 'learning': 4, 'CS': 5, 'Colby': 6}\n",
      "\n",
      "The reverse ind2word mapping is:\n",
      "{0: 'I', 1: 'love', 2: 'CS444', 3: 'deep', 4: 'learning', 5: 'CS', 6: 'Colby'}\n",
      "and it should be\n",
      "{0: 'I', 1: 'love', 2: 'CS444', 3: 'deep', 4: 'learning', 5: 'CS', 6: 'Colby'}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(f'For the test text, your word2ind mapping is:\\n{make_word2ind_mapping(unique)}')\n",
    "print('and it should be')\n",
    "print(\"{'I': 0, 'love': 1, 'CS444': 2, 'deep': 3, 'learning': 4, 'CS': 5, 'Colby': 6}\")\n",
    "print()\n",
    "print(f'The reverse ind2word mapping is:\\n{make_ind2word_mapping(unique)}')\n",
    "print('and it should be')\n",
    "print(\"{0: 'I', 1: 'love', 2: 'CS444', 3: 'deep', 4: 'learning', 5: 'CS', 6: 'Colby'}\")\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For the Amazon text, your word2ind mapping has :\n",
      "677 entries.\n",
      "and it should have\n",
      "677 entries.\n",
      "{'I': 0, 'love': 1, 'CS444': 2, 'deep': 3, 'learning': 4, 'CS': 5, 'Colby': 6}\n",
      "\n",
      "For the Amazon text, the reverse ind2word mapping has:\n",
      "677 entries.\n",
      "and it should have\n",
      "677 entries.\n"
     ]
    }
   ],
   "source": [
    "print(f'For the Amazon text, your word2ind mapping has :\\n{len(make_word2ind_mapping(unique_words_corpus))} entries.')\n",
    "print('and it should have\\n677 entries.')\n",
    "print(\"{'I': 0, 'love': 1, 'CS444': 2, 'deep': 3, 'learning': 4, 'CS': 5, 'Colby': 6}\")\n",
    "print()\n",
    "print(f'For the Amazon text, the reverse ind2word mapping has:\\n{len(make_ind2word_mapping(unique_words_corpus))} entries.')\n",
    "print('and it should have\\n677 entries.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1e. Use the corpus to form context and target word tensors\n",
    "\n",
    "Together, these make up the data samples and labels on which we will train the CBOW network. To form these tensors, we step through the corpus and identify each word as a target word and surrounding words within the **context window** as context words. To handle the fact that there are an irregular number of context words around each target words, we code samples as individual *pairs* of context and target words and we duplicate target words as needed. For example, for the target word `love` in `I love CS444`, we code add the target word entries of `1, 1` and add context word entries of `0, 2`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from amazon_reviews import make_target_context_word_lists"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test: context window of 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For the test text, the int-coded target words are:\n",
      "[0 1 1 2 0 1 1 3 3 4 0 1 1 5 0 1 1 6]\n",
      "and they should be:\n",
      "[0 1 1 2 0 1 1 3 3 4 0 1 1 5 0 1 1 6]\n",
      "For the test text, the int-coded context words are:\n",
      "[1 0 2 1 1 0 3 1 4 3 1 0 5 1 1 0 6 1]\n",
      "and they should be:\n",
      "[1 0 2 1 1 0 3 1 4 3 1 0 5 1 1 0 6 1]\n"
     ]
    }
   ],
   "source": [
    "test_target_words_int, test_context_words_int = make_target_context_word_lists(test_text,\n",
    "                                                                               make_word2ind_mapping(unique),\n",
    "                                                                               context_win_sz=1)\n",
    "print(f'For the test text, the int-coded target words are:\\n{test_target_words_int.numpy()}')\n",
    "print('and they should be:')\n",
    "print('[0 1 1 2 0 1 1 3 3 4 0 1 1 5 0 1 1 6]')\n",
    "print(f'For the test text, the int-coded context words are:\\n{test_context_words_int.numpy()}')\n",
    "print('and they should be:')\n",
    "print('[1 0 2 1 1 0 3 1 4 3 1 0 5 1 1 0 6 1]')\n",
    "# Make sure we have the correct dtype since these will eventually need to serve as indices\n",
    "assert test_target_words_int.dtype == tf.int32\n",
    "assert test_context_words_int.dtype == tf.int32"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test: Default context window of 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For the test text, the int-coded target words are:\n",
      "[0 0 1 1 2 2 0 0 1 1 1 3 3 3 4 4 0 0 1 1 5 5 0 0 1 1 6 6]\n",
      "and they should be:\n",
      "[0 0 1 1 2 2 0 0 1 1 1 3 3 3 4 4 0 0 1 1 5 5 0 0 1 1 6 6]\n",
      "For the test text, the int-coded context words are:\n",
      "[1 2 0 2 0 1 1 3 0 3 4 0 1 4 1 3 1 5 0 5 0 1 1 6 0 6 0 1]\n",
      "and they should be:\n",
      "[1 2 0 2 0 1 1 3 0 3 4 0 1 4 1 3 1 5 0 5 0 1 1 6 0 6 0 1]\n"
     ]
    }
   ],
   "source": [
    "test_target_words_int, test_context_words_int = make_target_context_word_lists(test_text,\n",
    "                                                                               make_word2ind_mapping(unique))\n",
    "print(f'For the test text, the int-coded target words are:\\n{test_target_words_int.numpy()}')\n",
    "print('and they should be:')\n",
    "print('[0 0 1 1 2 2 0 0 1 1 1 3 3 3 4 4 0 0 1 1 5 5 0 0 1 1 6 6]')\n",
    "print(f'For the test text, the int-coded context words are:\\n{test_context_words_int.numpy()}')\n",
    "print('and they should be:')\n",
    "print('[1 2 0 2 0 1 1 3 0 3 4 0 1 4 1 3 1 5 0 5 0 1 1 6 0 6 0 1]')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test: Amazon reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For the test text, there are 6930 int-coded target words and there should be 6930.\n",
      "For the test text, there are 6930 int-coded target words and there should be 6930.\n"
     ]
    }
   ],
   "source": [
    "am_target_words_int, am_context_words_int = make_target_context_word_lists(corpus,\n",
    "                                                                           make_word2ind_mapping(unique_words_corpus))\n",
    "print(f'For the test text, there are {len(am_target_words_int)} int-coded target words and there should be 6930.')\n",
    "print(f'For the test text, there are {len(am_context_words_int)} int-coded target words and there should be 6930.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1f. Function to automate obtaining training samples and labels\n",
    "\n",
    "Write `get_dataset_word2vec` to to streamline the process of going from the data file to getting the target and context word tensors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "from amazon_reviews import get_dataset_word2vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of target words in actual Amazon corpus: 5434338. There should be 5434338.\n",
      "Number of context words in actual Amazon corpus: 5434338. There should be 5434338.\n",
      "Vocab size in actual Amazon corpus: 21905. It should be 21905.\n"
     ]
    }
   ],
   "source": [
    "targets_int, contexts_int, vocab = get_dataset_word2vec(N_reviews=40000)\n",
    "print(f'Number of target words in actual Amazon corpus: {len(targets_int)}. There should be 5434338.')\n",
    "print(f'Number of context words in actual Amazon corpus: {len(contexts_int)}. There should be 5434338.')\n",
    "print(f'Vocab size in actual Amazon corpus: {len(vocab)}. It should be 21905.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 2: Build and train CBOW on Amazon Fashion Reviews\n",
    "\n",
    "Now that we have the Amazon data in an appropriate format, let's develop the CBOW neural network. It should take far less time to develop this net due to its simplicity."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2a. Copy over your deep learning library from Project 2\n",
    "\n",
    "Files to include are: `layers.py`, `block.py`, `network.py`, `tf_util.py`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2b. Create the `DenseEmbedding` layer\n",
    "\n",
    "This is the only new layer for CBOW, which itself is essentially just a `Dense` layer, but its inputs are used to *index* rather than *multiply* its weights. Implement the `DenseEmbedding` layer in `cbow_layers.py` then test your work below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "from cbow_layers import DenseEmbedding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test: `DenseEmbedding` layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of wts/bias is (10, 5)/(5,) and they should be (10, 5)/(5,)\n",
      "The netActs from forward pass:\n",
      "[[-0.1744 -0.5512 -0.1061 -0.3297  0.3191]\n",
      " [-0.1547 -0.2417 -0.328  -0.3959  0.0067]\n",
      " [ 0.1275 -0.3441 -0.02    0.4227  0.2251]]\n",
      "and they should be:\n",
      "[[-0.1744 -0.5512 -0.1061 -0.3297  0.3191]\n",
      " [-0.1547 -0.2417 -0.328  -0.3959  0.0067]\n",
      " [ 0.1275 -0.3441 -0.02    0.4227  0.2251]]\n"
     ]
    }
   ],
   "source": [
    "tf.random.set_seed(0)\n",
    "test_embed = DenseEmbedding('TestEmbedLayer', units=5)\n",
    "tf.random.set_seed(1)\n",
    "test_M = 10\n",
    "test_embed(tf.random.uniform(shape=(1, test_M)))\n",
    "print(f'Shape of wts/bias is {test_embed.get_wts().shape}/{test_embed.get_b().shape} and they should be (10, 5)/(5,)')\n",
    "test_inds = tf.constant([2, 1, 0], dtype=tf.int32)\n",
    "test_acts = test_embed(test_inds)\n",
    "print(f'The netActs from forward pass:\\n{test_acts.numpy()}')\n",
    "print('and they should be:')\n",
    "print('''[[-0.1744 -0.5512 -0.1061 -0.3297  0.3191]\n",
    " [-0.1547 -0.2417 -0.328  -0.3959  0.0067]\n",
    " [ 0.1275 -0.3441 -0.02    0.4227  0.2251]]''')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2c. Build CBOW architecture and implement forward pass\n",
    "\n",
    "The CBOW network has the following structure:\n",
    "\n",
    "Input → DenseEmbedding → Dense\n",
    "\n",
    "Both the input and output layer have `vocab_sz` units. The output layer uses regular softmax activation.\n",
    "\n",
    "Implement the following methods in the `CBOW` class in `cbow.py`:\n",
    "- constructor\n",
    "- `__call__`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [],
   "source": [
    "from cbow import CBOW"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test: `CBOW` forward pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------------------------------------------------\n",
      "Dense layer output(Output) shape: [1, 5]\n",
      "Dense layer output(Hidden) shape: [1, 3]\n",
      "---------------------------------------------------------------------------\n",
      "The CBOW output layer netActs from the test indices are:\n",
      "[[[0.1759 0.3235 0.0784 0.1804 0.2418]\n",
      "  [0.1438 0.1019 0.2473 0.3665 0.1406]\n",
      "  [0.2514 0.2696 0.1495 0.1182 0.2113]\n",
      "  [0.261  0.1628 0.2505 0.1626 0.1631]]]\n",
      "and they should be:\n",
      "[[[0.1759 0.3235 0.0784 0.1804 0.2418]\n",
      "  [0.1438 0.1019 0.2473 0.3665 0.1406]\n",
      "  [0.2514 0.2696 0.1495 0.1182 0.2113]\n",
      "  [0.261  0.1628 0.2505 0.1626 0.1631]]]\n"
     ]
    }
   ],
   "source": [
    "tf.random.set_seed(0)\n",
    "test_cbow = CBOW(C=5, input_feats_shape=(5,), embedding_dim=3)\n",
    "test_cbow.compile()\n",
    "test_inds = tf.constant([[1, 2, 3, 0]], dtype=tf.int32)\n",
    "test_acts = test_cbow(test_inds)\n",
    "print(f'The CBOW output layer netActs from the test indices are:\\n{test_acts.numpy()}')\n",
    "print('and they should be:')\n",
    "print('''[[[0.1759 0.3235 0.0784 0.1804 0.2418]\n",
    "  [0.1438 0.1019 0.2473 0.3665 0.1406]\n",
    "  [0.2514 0.2696 0.1495 0.1182 0.2113]\n",
    "  [0.261  0.1628 0.2505 0.1626 0.1631]]]''')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The compile summary from the cell above should look like:\n",
    "```\n",
    "---------------------------------------------------------------------------\n",
    "Dense layer output(Output) shape: [1, 5]\n",
    "Dense layer output(Hidden) shape: [1, 3]\n",
    "---------------------------------------------------------------------------\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2d. Implement CBOW `fit`\n",
    "\n",
    "Implement the CBOW `fit` method. This is a large-scale simplification of your existing `fit` method, since there is no validation set and no early stopping of any kind! I suggest copy-pasting your `DeepNetwork` fit method and paring down from there."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test: Amazon dev set\n",
    "\n",
    "Test out your `fit` method on a dev set created from the 1st 100 Amazon Fashion samples. Train a CBOW network with default hyperparameters for `100` epochs. The loss should drop from ~7.2 and stabilize to ~3.5."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------------------------------------------------\n",
      "Dense layer output(Output) shape: [1, 1356]\n",
      "Dense layer output(Hidden) shape: [1, 96]\n",
      "---------------------------------------------------------------------------\n",
      "Epoch 1: Training Loss = 7.2040\n",
      "Epoch 1/100 took 0.5107 seconds\n",
      "Epoch 2: Training Loss = 7.1835\n",
      "Epoch 2/100 took 0.0943 seconds\n",
      "Epoch 3: Training Loss = 7.1557\n",
      "Epoch 3/100 took 0.0831 seconds\n",
      "Epoch 4: Training Loss = 7.1196\n",
      "Epoch 4/100 took 0.0729 seconds\n",
      "Epoch 5: Training Loss = 7.0696\n",
      "Epoch 5/100 took 0.0749 seconds\n",
      "Epoch 6: Training Loss = 7.0066\n",
      "Epoch 6/100 took 0.0836 seconds\n",
      "Epoch 7: Training Loss = 6.9199\n",
      "Epoch 7/100 took 0.0941 seconds\n",
      "Epoch 8: Training Loss = 6.8173\n",
      "Epoch 8/100 took 0.0962 seconds\n",
      "Epoch 9: Training Loss = 6.6990\n",
      "Epoch 9/100 took 0.0885 seconds\n",
      "Epoch 10: Training Loss = 6.5699\n",
      "Epoch 10/100 took 0.0733 seconds\n",
      "Epoch 11: Training Loss = 6.4343\n",
      "Epoch 11/100 took 0.0766 seconds\n",
      "Epoch 12: Training Loss = 6.2881\n",
      "Epoch 12/100 took 0.0785 seconds\n",
      "Epoch 13: Training Loss = 6.1580\n",
      "Epoch 13/100 took 0.0794 seconds\n",
      "Epoch 14: Training Loss = 6.0553\n",
      "Epoch 14/100 took 0.0723 seconds\n",
      "Epoch 15: Training Loss = 5.9781\n",
      "Epoch 15/100 took 0.0736 seconds\n",
      "Epoch 16: Training Loss = 5.8985\n",
      "Epoch 16/100 took 0.0829 seconds\n",
      "Epoch 17: Training Loss = 5.8615\n",
      "Epoch 17/100 took 0.0947 seconds\n",
      "Epoch 18: Training Loss = 5.8376\n",
      "Epoch 18/100 took 0.0906 seconds\n",
      "Epoch 19: Training Loss = 5.8086\n",
      "Epoch 19/100 took 0.0882 seconds\n",
      "Epoch 20: Training Loss = 5.7579\n",
      "Epoch 20/100 took 0.0695 seconds\n",
      "Epoch 21: Training Loss = 5.7834\n",
      "Epoch 21/100 took 0.0737 seconds\n",
      "Epoch 22: Training Loss = 5.7660\n",
      "Epoch 22/100 took 0.0787 seconds\n",
      "Epoch 23: Training Loss = 5.7463\n",
      "Epoch 23/100 took 0.0928 seconds\n",
      "Epoch 24: Training Loss = 5.7063\n",
      "Epoch 24/100 took 0.0872 seconds\n",
      "Epoch 25: Training Loss = 5.7160\n",
      "Epoch 25/100 took 0.0911 seconds\n",
      "Epoch 26: Training Loss = 5.6935\n",
      "Epoch 26/100 took 0.1068 seconds\n",
      "Epoch 27: Training Loss = 5.7058\n",
      "Epoch 27/100 took 0.1227 seconds\n",
      "Epoch 28: Training Loss = 5.6474\n",
      "Epoch 28/100 took 0.1137 seconds\n",
      "Epoch 29: Training Loss = 5.6747\n",
      "Epoch 29/100 took 0.1238 seconds\n",
      "Epoch 30: Training Loss = 5.6558\n",
      "Epoch 30/100 took 0.0979 seconds\n",
      "Epoch 31: Training Loss = 5.6337\n",
      "Epoch 31/100 took 0.0990 seconds\n",
      "Epoch 32: Training Loss = 5.6221\n",
      "Epoch 32/100 took 0.1009 seconds\n",
      "Epoch 33: Training Loss = 5.6033\n",
      "Epoch 33/100 took 0.1000 seconds\n",
      "Epoch 34: Training Loss = 5.5948\n",
      "Epoch 34/100 took 0.0998 seconds\n",
      "Epoch 35: Training Loss = 5.5807\n",
      "Epoch 35/100 took 0.1113 seconds\n",
      "Epoch 36: Training Loss = 5.5501\n",
      "Epoch 36/100 took 0.1117 seconds\n",
      "Epoch 37: Training Loss = 5.5497\n",
      "Epoch 37/100 took 0.1140 seconds\n",
      "Epoch 38: Training Loss = 5.5229\n",
      "Epoch 38/100 took 0.1180 seconds\n",
      "Epoch 39: Training Loss = 5.5226\n",
      "Epoch 39/100 took 0.1237 seconds\n",
      "Epoch 40: Training Loss = 5.5064\n",
      "Epoch 40/100 took 0.1023 seconds\n",
      "Epoch 41: Training Loss = 5.4907\n",
      "Epoch 41/100 took 0.1036 seconds\n",
      "Epoch 42: Training Loss = 5.4828\n",
      "Epoch 42/100 took 0.1073 seconds\n",
      "Epoch 43: Training Loss = 5.4693\n",
      "Epoch 43/100 took 0.1150 seconds\n",
      "Epoch 44: Training Loss = 5.4484\n",
      "Epoch 44/100 took 0.1076 seconds\n",
      "Epoch 45: Training Loss = 5.4343\n",
      "Epoch 45/100 took 0.0990 seconds\n",
      "Epoch 46: Training Loss = 5.4215\n",
      "Epoch 46/100 took 0.1027 seconds\n",
      "Epoch 47: Training Loss = 5.3956\n",
      "Epoch 47/100 took 0.0988 seconds\n",
      "Epoch 48: Training Loss = 5.3614\n",
      "Epoch 48/100 took 0.1105 seconds\n",
      "Epoch 49: Training Loss = 5.3303\n",
      "Epoch 49/100 took 0.1206 seconds\n",
      "Epoch 50: Training Loss = 5.3351\n",
      "Epoch 50/100 took 0.0957 seconds\n",
      "Epoch 51: Training Loss = 5.3311\n",
      "Epoch 51/100 took 0.0816 seconds\n",
      "Epoch 52: Training Loss = 5.3390\n",
      "Epoch 52/100 took 0.0732 seconds\n",
      "Epoch 53: Training Loss = 5.2813\n",
      "Epoch 53/100 took 0.0758 seconds\n",
      "Epoch 54: Training Loss = 5.2724\n",
      "Epoch 54/100 took 0.0857 seconds\n",
      "Epoch 55: Training Loss = 5.2414\n",
      "Epoch 55/100 took 0.0837 seconds\n",
      "Epoch 56: Training Loss = 5.2177\n",
      "Epoch 56/100 took 0.0715 seconds\n",
      "Epoch 57: Training Loss = 5.1934\n",
      "Epoch 57/100 took 0.0752 seconds\n",
      "Epoch 58: Training Loss = 5.1676\n",
      "Epoch 58/100 took 0.0855 seconds\n",
      "Epoch 59: Training Loss = 5.1633\n",
      "Epoch 59/100 took 0.0791 seconds\n",
      "Epoch 60: Training Loss = 5.1519\n",
      "Epoch 60/100 took 0.0727 seconds\n",
      "Epoch 61: Training Loss = 5.1256\n",
      "Epoch 61/100 took 0.0819 seconds\n",
      "Epoch 62: Training Loss = 5.1143\n",
      "Epoch 62/100 took 0.1052 seconds\n",
      "Epoch 63: Training Loss = 5.0981\n",
      "Epoch 63/100 took 0.1094 seconds\n",
      "Epoch 64: Training Loss = 5.0554\n",
      "Epoch 64/100 took 0.0880 seconds\n",
      "Epoch 65: Training Loss = 5.0508\n",
      "Epoch 65/100 took 0.0723 seconds\n",
      "Epoch 66: Training Loss = 5.0208\n",
      "Epoch 66/100 took 0.0802 seconds\n",
      "Epoch 67: Training Loss = 5.0272\n",
      "Epoch 67/100 took 0.0906 seconds\n",
      "Epoch 68: Training Loss = 5.0140\n",
      "Epoch 68/100 took 0.1052 seconds\n",
      "Epoch 69: Training Loss = 4.9894\n",
      "Epoch 69/100 took 0.1049 seconds\n",
      "Epoch 70: Training Loss = 4.9404\n",
      "Epoch 70/100 took 0.0836 seconds\n",
      "Epoch 71: Training Loss = 4.9287\n",
      "Epoch 71/100 took 0.0705 seconds\n",
      "Epoch 72: Training Loss = 4.9028\n",
      "Epoch 72/100 took 0.0723 seconds\n",
      "Epoch 73: Training Loss = 4.8994\n",
      "Epoch 73/100 took 0.0822 seconds\n",
      "Epoch 74: Training Loss = 4.8759\n",
      "Epoch 74/100 took 0.0904 seconds\n",
      "Epoch 75: Training Loss = 4.8784\n",
      "Epoch 75/100 took 0.0934 seconds\n",
      "Epoch 76: Training Loss = 4.8457\n",
      "Epoch 76/100 took 0.0743 seconds\n",
      "Epoch 77: Training Loss = 4.8230\n",
      "Epoch 77/100 took 0.0712 seconds\n",
      "Epoch 78: Training Loss = 4.8178\n",
      "Epoch 78/100 took 0.0787 seconds\n",
      "Epoch 79: Training Loss = 4.7683\n",
      "Epoch 79/100 took 0.0891 seconds\n",
      "Epoch 80: Training Loss = 4.7667\n",
      "Epoch 80/100 took 0.0913 seconds\n",
      "Epoch 81: Training Loss = 4.7241\n",
      "Epoch 81/100 took 0.0921 seconds\n",
      "Epoch 82: Training Loss = 4.7437\n",
      "Epoch 82/100 took 0.0836 seconds\n",
      "Epoch 83: Training Loss = 4.7115\n",
      "Epoch 83/100 took 0.0697 seconds\n",
      "Epoch 84: Training Loss = 4.6888\n",
      "Epoch 84/100 took 0.1012 seconds\n",
      "Epoch 85: Training Loss = 4.6719\n",
      "Epoch 85/100 took 0.0942 seconds\n",
      "Epoch 86: Training Loss = 4.6436\n",
      "Epoch 86/100 took 0.0922 seconds\n",
      "Epoch 87: Training Loss = 4.6404\n",
      "Epoch 87/100 took 0.0920 seconds\n",
      "Epoch 88: Training Loss = 4.6206\n",
      "Epoch 88/100 took 0.0986 seconds\n",
      "Epoch 89: Training Loss = 4.6089\n",
      "Epoch 89/100 took 0.1229 seconds\n",
      "Epoch 90: Training Loss = 4.5863\n",
      "Epoch 90/100 took 0.1214 seconds\n",
      "Epoch 91: Training Loss = 4.5461\n",
      "Epoch 91/100 took 0.1079 seconds\n",
      "Epoch 92: Training Loss = 4.5692\n",
      "Epoch 92/100 took 0.1119 seconds\n",
      "Epoch 93: Training Loss = 4.5345\n",
      "Epoch 93/100 took 0.1221 seconds\n",
      "Epoch 94: Training Loss = 4.5113\n",
      "Epoch 94/100 took 0.1230 seconds\n",
      "Epoch 95: Training Loss = 4.5002\n",
      "Epoch 95/100 took 0.1278 seconds\n",
      "Epoch 96: Training Loss = 4.4751\n",
      "Epoch 96/100 took 0.1286 seconds\n",
      "Epoch 97: Training Loss = 4.4315\n",
      "Epoch 97/100 took 0.1272 seconds\n",
      "Epoch 98: Training Loss = 4.4326\n",
      "Epoch 98/100 took 0.1221 seconds\n",
      "Epoch 99: Training Loss = 4.4461\n",
      "Epoch 99/100 took 0.1262 seconds\n",
      "Epoch 100: Training Loss = 4.3895\n",
      "Epoch 100/100 took 0.1045 seconds\n",
      "Finished training after 100 epochs.\n"
     ]
    }
   ],
   "source": [
    "tf.random.set_seed(0)\n",
    "target_words, context_words, vocab = get_dataset_word2vec(N_reviews=100, verbose=False)\n",
    "vocab_sz = len(vocab)\n",
    "model = CBOW(C=vocab_sz, input_feats_shape=(vocab_sz,))\n",
    "model.compile()\n",
    "train_loss_hist = model.fit(x=context_words, y=target_words, epochs=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2e. Train CBOW on the Amazon reviews\n",
    "\n",
    "In the cell below, train CBOW on the first `40,000` Amazon Fashion reviews for `35` epochs. You should use defaults for the other hyperparameters.\n",
    "\n",
    "Create a plot showing the CBOW loss history over training epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------------------------------------------------\n",
      "Dense layer output(Output) shape: [1, 21905]\n",
      "Dense layer output(Hidden) shape: [1, 96]\n",
      "---------------------------------------------------------------------------\n",
      "Epoch 1: Training Loss = 6.3375\n",
      "Epoch 1/35 took 251.0212 seconds\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[157], line 6\u001b[0m\n\u001b[1;32m      4\u001b[0m model \u001b[38;5;241m=\u001b[39m CBOW(C\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mlen\u001b[39m(vocab), input_feats_shape\u001b[38;5;241m=\u001b[39m(\u001b[38;5;28mlen\u001b[39m(vocab),))\n\u001b[1;32m      5\u001b[0m model\u001b[38;5;241m.\u001b[39mcompile()\n\u001b[0;32m----> 6\u001b[0m train_loss_hist \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcontext_words\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtarget_words\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m35\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Desktop/Developer/WordEmbeddings/cbow.py:110\u001b[0m, in \u001b[0;36mCBOW.fit\u001b[0;34m(self, x, y, batch_size, epochs, print_every, verbose)\u001b[0m\n\u001b[1;32m    107\u001b[0m     x_batch, y_batch \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mgather(x, indices), tf\u001b[38;5;241m.\u001b[39mgather(y, indices)\n\u001b[1;32m    109\u001b[0m     \u001b[38;5;66;03m# Perform training step\u001b[39;00m\n\u001b[0;32m--> 110\u001b[0m     cur_loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx_batch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_batch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    111\u001b[0m     batch_losses\u001b[38;5;241m.\u001b[39mappend(cur_loss)\n\u001b[1;32m    113\u001b[0m \u001b[38;5;66;03m# Compute average loss for epoch\u001b[39;00m\n",
      "File \u001b[0;32m~/miniforge3/envs/cs444/lib/python3.10/site-packages/tensorflow/python/util/traceback_utils.py:150\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    148\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    149\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 150\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    151\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    152\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[0;32m~/miniforge3/envs/cs444/lib/python3.10/site-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py:833\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    830\u001b[0m compiler \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mxla\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnonXla\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    832\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m OptionalXlaContext(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile):\n\u001b[0;32m--> 833\u001b[0m   result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    835\u001b[0m new_tracing_count \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexperimental_get_tracing_count()\n\u001b[1;32m    836\u001b[0m without_tracing \u001b[38;5;241m=\u001b[39m (tracing_count \u001b[38;5;241m==\u001b[39m new_tracing_count)\n",
      "File \u001b[0;32m~/miniforge3/envs/cs444/lib/python3.10/site-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py:869\u001b[0m, in \u001b[0;36mFunction._call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    866\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock\u001b[38;5;241m.\u001b[39mrelease()\n\u001b[1;32m    867\u001b[0m   \u001b[38;5;66;03m# In this case we have created variables on the first call, so we run the\u001b[39;00m\n\u001b[1;32m    868\u001b[0m   \u001b[38;5;66;03m# defunned version which is guaranteed to never create variables.\u001b[39;00m\n\u001b[0;32m--> 869\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtracing_compilation\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall_function\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    870\u001b[0m \u001b[43m      \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_no_variable_creation_config\u001b[49m\n\u001b[1;32m    871\u001b[0m \u001b[43m  \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    872\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_variable_creation_config \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    873\u001b[0m   \u001b[38;5;66;03m# Release the lock early so that multiple threads can perform the call\u001b[39;00m\n\u001b[1;32m    874\u001b[0m   \u001b[38;5;66;03m# in parallel.\u001b[39;00m\n\u001b[1;32m    875\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock\u001b[38;5;241m.\u001b[39mrelease()\n",
      "File \u001b[0;32m~/miniforge3/envs/cs444/lib/python3.10/site-packages/tensorflow/python/eager/polymorphic_function/tracing_compilation.py:139\u001b[0m, in \u001b[0;36mcall_function\u001b[0;34m(args, kwargs, tracing_options)\u001b[0m\n\u001b[1;32m    137\u001b[0m bound_args \u001b[38;5;241m=\u001b[39m function\u001b[38;5;241m.\u001b[39mfunction_type\u001b[38;5;241m.\u001b[39mbind(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    138\u001b[0m flat_inputs \u001b[38;5;241m=\u001b[39m function\u001b[38;5;241m.\u001b[39mfunction_type\u001b[38;5;241m.\u001b[39munpack_inputs(bound_args)\n\u001b[0;32m--> 139\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunction\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_flat\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# pylint: disable=protected-access\u001b[39;49;00m\n\u001b[1;32m    140\u001b[0m \u001b[43m    \u001b[49m\u001b[43mflat_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcaptured_inputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfunction\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcaptured_inputs\u001b[49m\n\u001b[1;32m    141\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniforge3/envs/cs444/lib/python3.10/site-packages/tensorflow/python/eager/polymorphic_function/concrete_function.py:1322\u001b[0m, in \u001b[0;36mConcreteFunction._call_flat\u001b[0;34m(self, tensor_inputs, captured_inputs)\u001b[0m\n\u001b[1;32m   1318\u001b[0m possible_gradient_type \u001b[38;5;241m=\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPossibleTapeGradientTypes(args)\n\u001b[1;32m   1319\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (possible_gradient_type \u001b[38;5;241m==\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPOSSIBLE_GRADIENT_TYPES_NONE\n\u001b[1;32m   1320\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m executing_eagerly):\n\u001b[1;32m   1321\u001b[0m   \u001b[38;5;66;03m# No tape is watching; skip to running the function.\u001b[39;00m\n\u001b[0;32m-> 1322\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_inference_function\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall_preflattened\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1323\u001b[0m forward_backward \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_select_forward_and_backward_functions(\n\u001b[1;32m   1324\u001b[0m     args,\n\u001b[1;32m   1325\u001b[0m     possible_gradient_type,\n\u001b[1;32m   1326\u001b[0m     executing_eagerly)\n\u001b[1;32m   1327\u001b[0m forward_function, args_with_tangents \u001b[38;5;241m=\u001b[39m forward_backward\u001b[38;5;241m.\u001b[39mforward()\n",
      "File \u001b[0;32m~/miniforge3/envs/cs444/lib/python3.10/site-packages/tensorflow/python/eager/polymorphic_function/atomic_function.py:216\u001b[0m, in \u001b[0;36mAtomicFunction.call_preflattened\u001b[0;34m(self, args)\u001b[0m\n\u001b[1;32m    214\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mcall_preflattened\u001b[39m(\u001b[38;5;28mself\u001b[39m, args: Sequence[core\u001b[38;5;241m.\u001b[39mTensor]) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[1;32m    215\u001b[0m \u001b[38;5;250m  \u001b[39m\u001b[38;5;124;03m\"\"\"Calls with flattened tensor inputs and returns the structured output.\"\"\"\u001b[39;00m\n\u001b[0;32m--> 216\u001b[0m   flat_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall_flat\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    217\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfunction_type\u001b[38;5;241m.\u001b[39mpack_output(flat_outputs)\n",
      "File \u001b[0;32m~/miniforge3/envs/cs444/lib/python3.10/site-packages/tensorflow/python/eager/polymorphic_function/atomic_function.py:251\u001b[0m, in \u001b[0;36mAtomicFunction.call_flat\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m    249\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m record\u001b[38;5;241m.\u001b[39mstop_recording():\n\u001b[1;32m    250\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_bound_context\u001b[38;5;241m.\u001b[39mexecuting_eagerly():\n\u001b[0;32m--> 251\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_bound_context\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall_function\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    252\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    253\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    254\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfunction_type\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mflat_outputs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    255\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    256\u001b[0m   \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    257\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m make_call_op_in_graph(\n\u001b[1;32m    258\u001b[0m         \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    259\u001b[0m         \u001b[38;5;28mlist\u001b[39m(args),\n\u001b[1;32m    260\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_bound_context\u001b[38;5;241m.\u001b[39mfunction_call_options\u001b[38;5;241m.\u001b[39mas_attrs(),\n\u001b[1;32m    261\u001b[0m     )\n",
      "File \u001b[0;32m~/miniforge3/envs/cs444/lib/python3.10/site-packages/tensorflow/python/eager/context.py:1552\u001b[0m, in \u001b[0;36mContext.call_function\u001b[0;34m(self, name, tensor_inputs, num_outputs)\u001b[0m\n\u001b[1;32m   1550\u001b[0m cancellation_context \u001b[38;5;241m=\u001b[39m cancellation\u001b[38;5;241m.\u001b[39mcontext()\n\u001b[1;32m   1551\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m cancellation_context \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 1552\u001b[0m   outputs \u001b[38;5;241m=\u001b[39m \u001b[43mexecute\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexecute\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1553\u001b[0m \u001b[43m      \u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecode\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mutf-8\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1554\u001b[0m \u001b[43m      \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_outputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1555\u001b[0m \u001b[43m      \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtensor_inputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1556\u001b[0m \u001b[43m      \u001b[49m\u001b[43mattrs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1557\u001b[0m \u001b[43m      \u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1558\u001b[0m \u001b[43m  \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1560\u001b[0m   outputs \u001b[38;5;241m=\u001b[39m execute\u001b[38;5;241m.\u001b[39mexecute_with_cancellation(\n\u001b[1;32m   1561\u001b[0m       name\u001b[38;5;241m.\u001b[39mdecode(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[1;32m   1562\u001b[0m       num_outputs\u001b[38;5;241m=\u001b[39mnum_outputs,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1566\u001b[0m       cancellation_manager\u001b[38;5;241m=\u001b[39mcancellation_context,\n\u001b[1;32m   1567\u001b[0m   )\n",
      "File \u001b[0;32m~/miniforge3/envs/cs444/lib/python3.10/site-packages/tensorflow/python/eager/execute.py:53\u001b[0m, in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     51\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     52\u001b[0m   ctx\u001b[38;5;241m.\u001b[39mensure_initialized()\n\u001b[0;32m---> 53\u001b[0m   tensors \u001b[38;5;241m=\u001b[39m \u001b[43mpywrap_tfe\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mTFE_Py_Execute\u001b[49m\u001b[43m(\u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_handle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mop_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     54\u001b[0m \u001b[43m                                      \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     55\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m core\u001b[38;5;241m.\u001b[39m_NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m     56\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "tf.random.set_seed(0)\n",
    "\n",
    "target_words, context_words, vocab = get_dataset_word2vec(N_reviews=40000, verbose=False)\n",
    "model = CBOW(C=len(vocab), input_feats_shape=(len(vocab),))\n",
    "model.compile()\n",
    "train_loss_hist = model.fit(x=context_words, y=target_words, epochs=35)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(train_loss_hist, label='Training Loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('CBOW Training Loss Over Epochs')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2f. Get and save embeddings\n",
    "\n",
    "To prevent having to retrain the CBOW network every time you want to access or analyze the word embeddings, I am providing a method `save_embeddings` that saves the network embeddings to disk in subfolder `export` in your working directory. \n",
    "\n",
    "<!-- **You will need to save your embeddings for sentiment analysis net week.**  -->\n",
    "\n",
    "Here is how you would load the embeddings in the future after saving them:\n",
    "\n",
    "```python\n",
    "loaded_embeddings = np.load('export/embeddings.npz')\n",
    "loaded_embeddings = loaded_embeddings['embeddings']\n",
    "```\n",
    "\n",
    "The `save_embeddings` method and forthcoming analysis will require getting all the embeddings from the network (`get_all_embeddings`) or only the embedding for one specific word (`get_word_embedding`). In `cbow.py`, write these methods."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test: `get_all_embeddings` and `get_word_embedding`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.random.set_seed(0)\n",
    "test_net = CBOW(input_feats_shape=(3,), C=3, embedding_dim=5)\n",
    "test_net.compile()\n",
    "print('All the embeddings are:')\n",
    "print(test_net.get_all_embeddings().numpy())\n",
    "print('and they should be:')\n",
    "print('''[[ 0.8724  0.2442 -0.2423 -0.5982 -0.7141]\n",
    " [ 0.2715 -0.0081  0.6864  0.3479  0.3462]\n",
    " [-0.4074 -0.25    0.4582 -0.4027 -0.5542]]''')\n",
    "print('The embedding for index 1 is')\n",
    "print(test_net.get_word_embedding(1).numpy())\n",
    "print('and it should be:')\n",
    "print('''[ 0.2715 -0.0081  0.6864  0.3479  0.3462]''')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run/adapt the following cell to save your embeddings to disk (`net` is your net trained on the reviews)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "net.save_embeddings()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 3: Visualizing word embeddings with t-SNE\n",
    "\n",
    "In this task, you will use the **cosine similarity** metric to find the words that have the most similar embedding to some query words of your choice. You will use the [t-SNE dimensionality reduction algorithm ](https://en.wikipedia.org/wiki/T-distributed_stochastic_neighbor_embedding) built into scikit-learn to visualize in 2D the relative positioning of the query word and words with the highest cosine similarity."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3a. Implement cosine similarity to get the most similar words to a query word\n",
    "\n",
    "Given a word that we are interested in, the cosine similarity will find the $k$ words that have the most similar embeddings to that of the query word.\n",
    "\n",
    "Here is a refresher on the cosine similarity equation:\n",
    "\n",
    "$$\n",
    "\\text{Cosine Similarity} = \\frac{(\\text{Wts})(\\vec{w})}{\\sqrt{\\sum_{j=1}^H (\\text{Wts}^T)_j^2} \\sqrt{\\sum_{j=1}^H w_j^2}}\n",
    "$$\n",
    "\n",
    "Where $\\text{Wts}$ are all the embeddings, $\\text{Wts}^T$ is the transpose of the embeddings, $\\vec{w}$ is the word embedding vector for the query word.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from amazon_reviews import get_most_similar_words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test: `get_most_similar_words`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_word_str2int = {\n",
    "                     'Waterville': 0,\n",
    "                     'Acadia': 1,\n",
    "                     'Camden': 2,\n",
    "                     'Portland': 3,\n",
    "                     'Boothbay': 4,\n",
    "                     'Bangor': 5,\n",
    "                     'Kennebunkport': 6\n",
    "                    }\n",
    "test_word_int2str = {\n",
    "                     0: 'Waterville',\n",
    "                     1: 'Acadia',\n",
    "                     2: 'Camden',\n",
    "                     3: 'Portland',\n",
    "                     4: 'Boothbay',\n",
    "                     5: 'Bangor',\n",
    "                     6: 'Kennebunkport'\n",
    "                    }\n",
    "\n",
    "test_word_str = 'Waterville'\n",
    "tf.random.set_seed(0)\n",
    "test_embeddings = tf.random.uniform(shape=(7, 4)).numpy()\n",
    "test_top_inds, test_top_sims = get_most_similar_words(k=3,\n",
    "                                                      all_embeddings=test_embeddings,\n",
    "                                                      word_str=test_word_str,\n",
    "                                                      word_str2int=test_word_str2int)\n",
    "\n",
    "print(f'Words most similar to {test_word_str}:')\n",
    "for k0 in range(len(test_top_inds)):\n",
    "    print(f'{k0}: {test_word_int2str[test_top_inds[k0]]} (similarity={test_top_sims[k0]:.4f})')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3b. Use t-SNE to perform dimensionality reduction on the embeddings.\n",
    "\n",
    "In the cell below:\n",
    "1. Use [scikit-learn's TSNE class](https://scikit-learn.org/stable/modules/generated/sklearn.manifold.TSNE.html) to reduce the dimensionality of the learned Amazon word embeddings down to 2D so that we will be able to visualize them in a scatter plot.\n",
    "2. Assign the original embeddings to a variable `embeddings`.\n",
    "3. Assign the 2D embeddings to a variable `word_tnse`. Make sure `embeddings` is a NumPy ndarray rather than a TF tensor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.manifold import TSNE\n",
    "from amazon_reviews import find_unique_word_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3c. Create word cloud from Amazon Fashion review embeddings\n",
    "\n",
    "In the cell below:\n",
    "\n",
    "1. Name the word string-to-int map `word2ind` and the word index int-to-string map `ind2word` for the Amazon Fashion dataset.\n",
    "2. Name the vocabulary for the Amazon Fashion dataset `vocab`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pick a word for `query_word` (has it be in the vocab) and run the cell below to show the words with the 25 most similar embeddings as well as a scatter plot, which shows all the word embeddings but annotates the query word and the most similar words.\n",
    "\n",
    "**Note:** At least to start, pick query words among those that appear the most in the corpus. Code two cells down lists these most frequent words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_word = 'great'\n",
    "topk_word_inds, topk_cossim = get_most_similar_words(k=25,\n",
    "                                                     all_embeddings=embeddings,\n",
    "                                                     word_str=query_word,\n",
    "                                                     word_str2int=word2ind)\n",
    "\n",
    "\n",
    "word_strs = [ind2word[ind] for ind in topk_word_inds]\n",
    "\n",
    "print(f'Words most similar to {query_word}:')\n",
    "for k0 in range(len(topk_word_inds)):\n",
    "    print(f'{k0}: {ind2word[topk_word_inds[k0]]} (similarity={topk_cossim[k0]:.4f})')\n",
    "\n",
    "# Visualize word cloud — each word as (x, y) coords\n",
    "plt.figure(figsize=(40, 40))\n",
    "plt.scatter(word_tnse[:, 0], word_tnse[:, 1])\n",
    "\n",
    "max_xy = 5\n",
    "rng = np.random.default_rng(0)\n",
    "offsets = rng.uniform(low=-max_xy, high=max_xy, size=(len(word_strs), 2))\n",
    "for w in range(len(word_strs)):\n",
    "    plt.annotate(word_strs[w], (word_tnse[w, 0]+offsets[w,0], word_tnse[w, 1]+offsets[w,0]), fontsize=30)\n",
    "\n",
    "plt.title('Word Embeddings (2D t-SNE)')\n",
    "plt.xlabel('Embedding dim 1')\n",
    "plt.ylabel('Embedding dim 2')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Print the words that appear most often in the Amazon Fashion corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: modify variable name `corpus` to refer to your Amazon Fashion corpus\n",
    "unique_word_counts = find_unique_word_counts(corpus=corpus)\n",
    "top_k = 100\n",
    "\n",
    "print(f'Top {top_k} words (by count in corpus):')\n",
    "i = 0\n",
    "for word, count in unique_word_counts.items():\n",
    "    print(word, count)\n",
    "    i += 1\n",
    "\n",
    "    if i > top_k:\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3d. Questions\n",
    "\n",
    "**Question 1:** Have some fun looking up word similarities. Which similarities do you like best / find the most interesting?\n",
    "\n",
    "**Question 2:** Do the most similar words tend to show up nearby or far from one another in the word cloud?\n",
    "\n",
    "**Question 3:** Why does the quality of the similar words improve for words that are more frequent in the corpus?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer 1:** \n",
    "\n",
    "**Answer 2:** \n",
    "\n",
    "**Answer 3:** "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extensions\n",
    "\n",
    "### General guidelines\n",
    "\n",
    "1. Never integrate extensions into your base project so that they change the expected behavior of core functions. If your extension changes the core design/behavior, no problem, duplicate your working base project and add features from there.\n",
    "2. Check the rubric to keep in mind how extensions on this project will be graded.\n",
    "3. While I may consult your code and \"written log\" of what you did, **I am grading your extensions based on what you present in your 3-5 min video.**\n",
    "3. I suggest documenting your explorations in a \"log\" or \"lab notebook\" style (i.e. documenting your thought/progression/discovery/learning process). I'm not grading your writing, so you can keep it succinct. **Whatever is most useful to you to remember what you did.** \n",
    "4. I suggest taking a hypothesis driven approach. For example \"I was curious about X so I explored Y. I found Z, which was not what I expected because..., so then tried A...\"\n",
    "5. Make plots to help showcase your results.\n",
    "6. **More is not necessarily better.** Generally, a small number of \"in-depth\" extensions count for more than many \"shallow\" extensions.\n",
    "\n",
    "### AI guidelines\n",
    "\n",
    "You may use AI in mostly any capacity for extensions. However, keep in mind:\n",
    "1. There is no need to use AI at all!\n",
    "2. You are welcome to use AI as a tool (e.g. automate something that is tedious, help you get unstuck, etc.). However, you should be coding, you should be thinking, you should be writing, you should be creating. If you are spending most (or even close to most) of your time typing into a chatbot and copy-pasting, you have probably gone too far with AI use.\n",
    "3. I don't find large volumes of AI generated code/text/plots to be particularly impressive and you risk losing my interest while grading. Remember: I'm grading your extensions based on your video presentation. **More is not necessarily better.**\n",
    "\n",
    "### Video guidelines\n",
    "\n",
    "1. Please try to keep your video to 5 minutes (*I have other projects to grade!*). If you turn in a longer video, I make no promise that I will watch more than 5 minutes.\n",
    "2. Your screen should be shared as you show me what you did. A live video of your face should also appear somewhere on the screen (e.g. picture-in-picture overlay / split screen).\n",
    "3. Your partner should join you for the video and take turns talking, but, if necessary, it is fine to have one team member present during the record the video.\n",
    "4. Do not simply read text from your notebook, do not read from a prepared script. I am not grading how polished your video presentation is (see extension grading criteria on rubric). \n",
    "5. I am looking for original and creative explorations sparked by your curiosity/interest/passion in a topic. This should be apparent in your video.\n",
    "6. Be natural,, don't feel the need to impress me with fancy language. If it is helpful, imagine that we are talking one-on-one about your extension. Tell me what you did :)\n",
    "\n",
    "### Extension ideas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. Analyze effect of embedding dimension\n",
    "\n",
    "- Systematically vary the embedding dimension. How does the embedding dimension affect the quality of similar words based on their embeddings / cosine similarity?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Amount of data\n",
    "\n",
    "How does the size of the dataset (number of reviews) affect the quality of the embeddings?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. Improve text preprocessing\n",
    "\n",
    "Try implementing and seeing how any of the following may change the quality of embeddings:\n",
    "\n",
    "**Stemming:** Currently words with different suffixes are treated the same — e.g. \"run\", \"runs\", \"running\", etc. Normalize these so they map the same word.\n",
    "\n",
    "**Remove stop words:** Remove common \"filler\" words that have little meaning — e.g. \"a\", \"the\", \"an\", etc.\n",
    "\n",
    "**Misspelled words**: There are numerous misspellings of words in the corpus. Does having them in the vocab help or hurt?\n",
    "\n",
    "If things improve/worsen, why might this be the case?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4. Other text dataset of your choice\n",
    "\n",
    "Obtain and preprocess a text dataset of your choice. Then either train on CBOW and visualize/analyze embeddings."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5. Sentiment analysis\n",
    "\n",
    "(*This is a more involved/challenging extension*) Use the word embedding vectors as data samples into another neural network whose job it is to predict whether either the word or the Amazon review to which it and other words in the review belong is positive (say >3 star rating) or negative (say < 3 star rating). \n",
    "\n",
    "For predicting the sentiment of words, you can use a simple heuristic to get the +/- label for individual words: If the word belongs to more + reviews than - reviews, then the word is a + word (and vice versa)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6. Other dimensionality reduction techniques\n",
    "\n",
    "Explore how different dimensionality reduction techniques alter the visualization/clustering of the words. *Keep in mind that many algorithms have hyperparameters and if one algorithm gives poor results, you should explore other hyperparameter values before concluding the algorithm does a poor job.*\n",
    "\n",
    "Some popular ideas:\n",
    "- PCA\n",
    "- UMAP\n",
    "- Self-organizing map (SOM)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cs444",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
