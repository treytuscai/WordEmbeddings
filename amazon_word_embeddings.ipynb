{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Trey Tuscai and Gordon Doore\n",
    "\n",
    "Spring 2025\n",
    "\n",
    "CS 444: Deep Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.style.use(['seaborn-v0_8-colorblind', 'seaborn-v0_8-darkgrid'])\n",
    "plt.rcParams.update({'font.size': 20})\n",
    "\n",
    "np.set_printoptions(suppress=True, precision=4)\n",
    "\n",
    "# Automatically reload your external source code\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- # Project 3 | Word Embeddings and Sentiment Analysis -->\n",
    "# Project 3 | Word Embeddings\n",
    "\n",
    "The goal of this project is to gain insight about the representations learned by neural networks — how do they encode the training data internally? We will transition to training neural network on text data — specifically on tens of thousands of real Amazon reviews of fashion products (e.g. clothes, shoes, etc.). \n",
    "\n",
    "<!-- Next week, you will train a CNN to predict whether the reviews are positive or negative, a task in the field natural language processing (NLP) called **sentiment analysis**. -->\n",
    "\n",
    "<!-- #### Week 1: Word Embeddings of Amazon Fashion Reviews -->\n",
    "\n",
    "This notebook focuses on building the Amazon Fashion Reviews text preprocessing pipeline as well as analyzing and visualizing how a neural network learns to encode words from the reviews. You will train a **Continuous Bag of Words (CBOW)** (word2vec) neural network commonly used in the field of natural language processing (NLP) on text from IMDb user movie reviews. The network attempts to predict a **target word** (a missing word from a passage of text) from the surrounding **context words** (the words surrounding the target word in a sentence). After implementing and training the network, you will extract the weights to obtain $H$ dimensional **word embedding** vectors for English words that appeared in the Amazon reviews to analyze and visualize. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 1: Preprocessing Amazon Fashion Reviews"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1a. Download and inspect first several few Amazon reviews\n",
    "\n",
    "Download the Amazon Fashion dataset (`Amazon_Fashion.jsonl`) from the project website and store it in the `data` subfolder of your working directory. These Amazon reviews were curated by [the McAuley lab in 2023](https://huggingface.co/datasets/McAuley-Lab/Amazon-Reviews-2023).\n",
    "\n",
    "In the cell below, use the provided `load_reviews_and_ratings` function in `amazon_reviews.py` to load in the 1st five reviews and ratings. Print these first five reviews and clearly indicate the star rating for each review."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from amazon_reviews import load_reviews_and_ratings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Review 1:\n",
      "Rating: 5.0 stars\n",
      "Text: I think this locket is really pretty. The inside back is a solid silver depression and the front is a dome that is not solid (knotted). You could use it to store a small photo, lock of hair, etc but I use it when I need to carry medication with me. Closes securely. High quality & very pretty.\n",
      "\n",
      "Review 2:\n",
      "Rating: 5.0 stars\n",
      "Text: Great\n",
      "\n",
      "Review 3:\n",
      "Rating: 2.0 stars\n",
      "Text: One of the stones fell out within the first 2 weeks of wearing it. Stones smaller than expected.\n",
      "\n",
      "Review 4:\n",
      "Rating: 1.0 stars\n",
      "Text: Crappy socks. Money wasted. Bought to wear with my tieks. Don’t stay on feet well.\n",
      "\n",
      "Review 5:\n",
      "Rating: 5.0 stars\n",
      "Text: I LOVE these glasses!  They fit perfectly over my regular, rectangular glasses that I always have to wear in order to see.  I really appreciate having these pretty and stylish and sturdy sunglasses to wear over my glasses.  I'll buy these again and again whenever I need a new pair, which hopefully won't be too soon.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "reviews, ratings = load_reviews_and_ratings(N_reviews=5)\n",
    "\n",
    "for i, (review, rating) in enumerate(zip(reviews, ratings)):\n",
    "    print(f\"Review {i+1}:\")\n",
    "    print(f\"Rating: {rating:.1f} stars\")\n",
    "    print(f\"Text: {review}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1b. Make the corpus\n",
    "\n",
    "We will use a \"flat\"/1D list of sentences across the reviews as our **corpus**. That is, we REMOVE information about which review a sentence came from in the corpus. However, we will eventually need this information in order to determine whether a review is positive/negative so we maintain a separate list that indicates to which review a given sentence belongs. For example: Sent 0 belongs to review 0, Sent 1 belongs to review 0, Sent 2 belongs to review 1, etc.\n",
    "\n",
    "To make sure we have enough context words in sentences, we prune sentences that are too short. We also prune sentences that are too long.\n",
    "\n",
    "Implement `make_corpus` in `amazon_reviews.py` to create the corpus of sentences and review ID associations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from amazon_reviews import make_corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of sentences in the corpus (after pruning short sentences/identifying words): 185.\n",
      "There should be 185 sentences\n",
      "The first few sent ratings are: [5. 5. 5. 5. 5. 2. 2. 1. 1. 1.] and should be [5. 5. 5. 5. 5. 2. 2. 1. 1. 1.]\n",
      "The last few sent ratings are: [2. 2. 2. 2. 2. 2. 2. 2. 2. 2.] and should be [2. 2. 2. 2. 2. 2. 2. 2. 2. 2.]\n",
      "The first few review_ids are: [0 0 0 0 0 2 2 3 3 3] and should be [0 0 0 0 0 2 2 3 3 3]\n",
      "The last few review_ids are: [49 49 49 49 49 49 49 49 49 49] and should be [49 49 49 49 49 49 49 49 49 49]\n"
     ]
    }
   ],
   "source": [
    "corpus, sentence_ratings, review_ids = make_corpus(N_reviews=50)\n",
    "print(f'Number of sentences in the corpus (after pruning short sentences/identifying words): {len(corpus)}.')\n",
    "print('There should be 185 sentences')\n",
    "assert len(corpus) == len(sentence_ratings)\n",
    "assert len(corpus) == len(review_ids)\n",
    "print(f'The first few sent ratings are: {sentence_ratings[:10]} and should be [5. 5. 5. 5. 5. 2. 2. 1. 1. 1.]')\n",
    "print(f'The last few sent ratings are: {sentence_ratings[-10:]} and should be [2. 2. 2. 2. 2. 2. 2. 2. 2. 2.]')\n",
    "print(f'The first few review_ids are: {review_ids[:10]} and should be [0 0 0 0 0 2 2 3 3 3]')\n",
    "print(f'The last few review_ids are: {review_ids[-10:]} and should be [49 49 49 49 49 49 49 49 49 49]')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1c. Make the vocabulary\n",
    "\n",
    "We will be training a **word-level model**, so our tokens will be single words. Implement `find_unique_words` to form the **vocabulary**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from amazon_reviews import find_unique_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "unique words:\n",
      " ['I', 'love', 'CS444', 'deep', 'learning', 'CS', 'Colby']\n",
      "unique words should be:\n",
      " ['I', 'love', 'CS444', 'deep', 'learning', 'CS', 'Colby']\n",
      "Number of unique words in the 50 reviews: 677 and should be 677.\n"
     ]
    }
   ],
   "source": [
    "test_text = [['I', 'love', 'CS444'],\n",
    "             ['I', 'love', 'deep', 'learning'],\n",
    "             ['I', 'love', 'CS'],\n",
    "             ['I', 'love', 'Colby']]\n",
    "unique = find_unique_words(test_text)\n",
    "print('unique words:\\n', unique)\n",
    "print(\"unique words should be:\\n ['I', 'love', 'CS444', 'deep', 'learning', 'CS', 'Colby']\")\n",
    "\n",
    "unique_words_corpus = find_unique_words(corpus)\n",
    "print(f'Number of unique words in the 50 reviews: {len(unique_words_corpus)} and should be 677.')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1d. Mapping words to indices and back\n",
    "\n",
    "While our corpus is currently made of word strings we clearly cannot plug these in as inputs to a neural network! We need to convert these word features to numbers first. For the specific CBOW word2vec neural network we are implementing, we *could plug in each word represented as a one-hot vector. To determine the location of the 1 in each vector, we use the word's position in the vocabulary. \n",
    "\n",
    "Implement and test `make_word2ind_mapping` to convert a string word to its position index in the vocab and `make_ind2word_mapping` to perform the reverse — looking up a string word based on its int index.\n",
    "\n",
    "*For efficiency, we won't *actually* plug in one-hot vectors into CBOW. We'll use the int indices instead."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from amazon_reviews import make_word2ind_mapping, make_ind2word_mapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For the test text, your word2ind mapping is:\n",
      "{'I': 0, 'love': 1, 'CS444': 2, 'deep': 3, 'learning': 4, 'CS': 5, 'Colby': 6}\n",
      "and it should be\n",
      "{'I': 0, 'love': 1, 'CS444': 2, 'deep': 3, 'learning': 4, 'CS': 5, 'Colby': 6}\n",
      "\n",
      "The reverse ind2word mapping is:\n",
      "{0: 'I', 1: 'love', 2: 'CS444', 3: 'deep', 4: 'learning', 5: 'CS', 6: 'Colby'}\n",
      "and it should be\n",
      "{0: 'I', 1: 'love', 2: 'CS444', 3: 'deep', 4: 'learning', 5: 'CS', 6: 'Colby'}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(f'For the test text, your word2ind mapping is:\\n{make_word2ind_mapping(unique)}')\n",
    "print('and it should be')\n",
    "print(\"{'I': 0, 'love': 1, 'CS444': 2, 'deep': 3, 'learning': 4, 'CS': 5, 'Colby': 6}\")\n",
    "print()\n",
    "print(f'The reverse ind2word mapping is:\\n{make_ind2word_mapping(unique)}')\n",
    "print('and it should be')\n",
    "print(\"{0: 'I', 1: 'love', 2: 'CS444', 3: 'deep', 4: 'learning', 5: 'CS', 6: 'Colby'}\")\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For the Amazon text, your word2ind mapping has :\n",
      "677 entries.\n",
      "and it should have\n",
      "677 entries.\n",
      "{'I': 0, 'love': 1, 'CS444': 2, 'deep': 3, 'learning': 4, 'CS': 5, 'Colby': 6}\n",
      "\n",
      "For the Amazon text, the reverse ind2word mapping has:\n",
      "677 entries.\n",
      "and it should have\n",
      "677 entries.\n"
     ]
    }
   ],
   "source": [
    "print(f'For the Amazon text, your word2ind mapping has :\\n{len(make_word2ind_mapping(unique_words_corpus))} entries.')\n",
    "print('and it should have\\n677 entries.')\n",
    "print(\"{'I': 0, 'love': 1, 'CS444': 2, 'deep': 3, 'learning': 4, 'CS': 5, 'Colby': 6}\")\n",
    "print()\n",
    "print(f'For the Amazon text, the reverse ind2word mapping has:\\n{len(make_ind2word_mapping(unique_words_corpus))} entries.')\n",
    "print('and it should have\\n677 entries.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1e. Use the corpus to form context and target word tensors\n",
    "\n",
    "Together, these make up the data samples and labels on which we will train the CBOW network. To form these tensors, we step through the corpus and identify each word as a target word and surrounding words within the **context window** as context words. To handle the fact that there are an irregular number of context words around each target words, we code samples as individual *pairs* of context and target words and we duplicate target words as needed. For example, for the target word `love` in `I love CS444`, we code add the target word entries of `1, 1` and add context word entries of `0, 2`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from amazon_reviews import make_target_context_word_lists"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test: context window of 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For the test text, the int-coded target words are:\n",
      "[0 1 1 2 0 1 1 3 3 4 0 1 1 5 0 1 1 6]\n",
      "and they should be:\n",
      "[0 1 1 2 0 1 1 3 3 4 0 1 1 5 0 1 1 6]\n",
      "For the test text, the int-coded context words are:\n",
      "[1 0 2 1 1 0 3 1 4 3 1 0 5 1 1 0 6 1]\n",
      "and they should be:\n",
      "[1 0 2 1 1 0 3 1 4 3 1 0 5 1 1 0 6 1]\n"
     ]
    }
   ],
   "source": [
    "test_target_words_int, test_context_words_int = make_target_context_word_lists(test_text,\n",
    "                                                                               make_word2ind_mapping(unique),\n",
    "                                                                               context_win_sz=1)\n",
    "print(f'For the test text, the int-coded target words are:\\n{test_target_words_int.numpy()}')\n",
    "print('and they should be:')\n",
    "print('[0 1 1 2 0 1 1 3 3 4 0 1 1 5 0 1 1 6]')\n",
    "print(f'For the test text, the int-coded context words are:\\n{test_context_words_int.numpy()}')\n",
    "print('and they should be:')\n",
    "print('[1 0 2 1 1 0 3 1 4 3 1 0 5 1 1 0 6 1]')\n",
    "# Make sure we have the correct dtype since these will eventually need to serve as indices\n",
    "assert test_target_words_int.dtype == tf.int32\n",
    "assert test_context_words_int.dtype == tf.int32"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test: Default context window of 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For the test text, the int-coded target words are:\n",
      "[0 0 1 1 2 2 0 0 1 1 1 3 3 3 4 4 0 0 1 1 5 5 0 0 1 1 6 6]\n",
      "and they should be:\n",
      "[0 0 1 1 2 2 0 0 1 1 1 3 3 3 4 4 0 0 1 1 5 5 0 0 1 1 6 6]\n",
      "For the test text, the int-coded context words are:\n",
      "[1 2 0 2 0 1 1 3 0 3 4 0 1 4 1 3 1 5 0 5 0 1 1 6 0 6 0 1]\n",
      "and they should be:\n",
      "[1 2 0 2 0 1 1 3 0 3 4 0 1 4 1 3 1 5 0 5 0 1 1 6 0 6 0 1]\n"
     ]
    }
   ],
   "source": [
    "test_target_words_int, test_context_words_int = make_target_context_word_lists(test_text,\n",
    "                                                                               make_word2ind_mapping(unique))\n",
    "print(f'For the test text, the int-coded target words are:\\n{test_target_words_int.numpy()}')\n",
    "print('and they should be:')\n",
    "print('[0 0 1 1 2 2 0 0 1 1 1 3 3 3 4 4 0 0 1 1 5 5 0 0 1 1 6 6]')\n",
    "print(f'For the test text, the int-coded context words are:\\n{test_context_words_int.numpy()}')\n",
    "print('and they should be:')\n",
    "print('[1 2 0 2 0 1 1 3 0 3 4 0 1 4 1 3 1 5 0 5 0 1 1 6 0 6 0 1]')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test: Amazon reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For the test text, there are 6930 int-coded target words and there should be 6930.\n",
      "For the test text, there are 6930 int-coded target words and there should be 6930.\n"
     ]
    }
   ],
   "source": [
    "am_target_words_int, am_context_words_int = make_target_context_word_lists(corpus,\n",
    "                                                                           make_word2ind_mapping(unique_words_corpus))\n",
    "print(f'For the test text, there are {len(am_target_words_int)} int-coded target words and there should be 6930.')\n",
    "print(f'For the test text, there are {len(am_context_words_int)} int-coded target words and there should be 6930.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1f. Function to automate obtaining training samples and labels\n",
    "\n",
    "Write `get_dataset_word2vec` to to streamline the process of going from the data file to getting the target and context word tensors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "from amazon_reviews import get_dataset_word2vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of target words in actual Amazon corpus: 5434338. There should be 5434338.\n",
      "Number of context words in actual Amazon corpus: 5434338. There should be 5434338.\n",
      "Vocab size in actual Amazon corpus: 21905. It should be 21905.\n"
     ]
    }
   ],
   "source": [
    "targets_int, contexts_int, vocab = get_dataset_word2vec(N_reviews=40000)\n",
    "print(f'Number of target words in actual Amazon corpus: {len(targets_int)}. There should be 5434338.')\n",
    "print(f'Number of context words in actual Amazon corpus: {len(contexts_int)}. There should be 5434338.')\n",
    "print(f'Vocab size in actual Amazon corpus: {len(vocab)}. It should be 21905.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 2: Build and train CBOW on Amazon Fashion Reviews\n",
    "\n",
    "Now that we have the Amazon data in an appropriate format, let's develop the CBOW neural network. It should take far less time to develop this net due to its simplicity."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2a. Copy over your deep learning library from Project 2\n",
    "\n",
    "Files to include are: `layers.py`, `block.py`, `network.py`, `tf_util.py`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2b. Create the `DenseEmbedding` layer\n",
    "\n",
    "This is the only new layer for CBOW, which itself is essentially just a `Dense` layer, but its inputs are used to *index* rather than *multiply* its weights. Implement the `DenseEmbedding` layer in `cbow_layers.py` then test your work below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from cbow_layers import DenseEmbedding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test: `DenseEmbedding` layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.random.set_seed(0)\n",
    "test_embed = DenseEmbedding('TestEmbedLayer', units=5)\n",
    "tf.random.set_seed(1)\n",
    "test_M = 10\n",
    "test_embed(tf.random.uniform(shape=(1, test_M)))\n",
    "print(f'Shape of wts/bias is {test_embed.get_wts().shape}/{test_embed.get_b().shape} and they should be (10, 5)/(5,)')\n",
    "test_inds = tf.constant([2, 1, 0], dtype=tf.int32)\n",
    "test_acts = test_embed(test_inds)\n",
    "print(f'The netActs from forward pass:\\n{test_acts.numpy()}')\n",
    "print('and they should be:')\n",
    "print('''[[-0.1744 -0.5512 -0.1061 -0.3297  0.3191]\n",
    " [-0.1547 -0.2417 -0.328  -0.3959  0.0067]\n",
    " [ 0.1275 -0.3441 -0.02    0.4227  0.2251]]''')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2c. Build CBOW architecture and implement forward pass\n",
    "\n",
    "The CBOW network has the following structure:\n",
    "\n",
    "Input → DenseEmbedding → Dense\n",
    "\n",
    "Both the input and output layer have `vocab_sz` units. The output layer uses regular softmax activation.\n",
    "\n",
    "Implement the following methods in the `CBOW` class in `cbow.py`:\n",
    "- constructor\n",
    "- `__call__`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from cbow import CBOW"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test: `CBOW` forward pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.random.set_seed(0)\n",
    "test_cbow = CBOW(C=5, input_feats_shape=(5,), embedding_dim=3)\n",
    "test_cbow.compile()\n",
    "test_inds = tf.constant([[1, 2, 3, 0]], dtype=tf.int32)\n",
    "test_acts = test_cbow(test_inds)\n",
    "print(f'The CBOW output layer netActs from the test indices are:\\n{test_acts.numpy()}')\n",
    "print('and they should be:')\n",
    "print('''[[[0.1759 0.3235 0.0784 0.1804 0.2418]\n",
    "  [0.1438 0.1019 0.2473 0.3665 0.1406]\n",
    "  [0.2514 0.2696 0.1495 0.1182 0.2113]\n",
    "  [0.261  0.1628 0.2505 0.1626 0.1631]]]''')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The compile summary from the cell above should look like:\n",
    "```\n",
    "---------------------------------------------------------------------------\n",
    "Dense layer output(Output) shape: [1, 5]\n",
    "Dense layer output(Hidden) shape: [1, 3]\n",
    "---------------------------------------------------------------------------\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2d. Implement CBOW `fit`\n",
    "\n",
    "Implement the CBOW `fit` method. This is a large-scale simplification of your existing `fit` method, since there is no validation set and no early stopping of any kind! I suggest copy-pasting your `DeepNetwork` fit method and paring down from there."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test: Amazon dev set\n",
    "\n",
    "Test out your `fit` method on a dev set created from the 1st 100 Amazon Fashion samples. Train a CBOW network with default hyperparameters for `100` epochs. The loss should drop from ~7.2 and stabilize to ~3.5."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.random.set_seed(0)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2e. Train CBOW on the Amazon reviews\n",
    "\n",
    "In the cell below, train CBOW on the first `40,000` Amazon Fashion reviews for `35` epochs. You should use defaults for the other hyperparameters.\n",
    "\n",
    "Create a plot showing the CBOW loss history over training epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.random.set_seed(0)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2f. Get and save embeddings\n",
    "\n",
    "To prevent having to retrain the CBOW network every time you want to access or analyze the word embeddings, I am providing a method `save_embeddings` that saves the network embeddings to disk in subfolder `export` in your working directory. \n",
    "\n",
    "<!-- **You will need to save your embeddings for sentiment analysis net week.**  -->\n",
    "\n",
    "Here is how you would load the embeddings in the future after saving them:\n",
    "\n",
    "```python\n",
    "loaded_embeddings = np.load('export/embeddings.npz')\n",
    "loaded_embeddings = loaded_embeddings['embeddings']\n",
    "```\n",
    "\n",
    "The `save_embeddings` method and forthcoming analysis will require getting all the embeddings from the network (`get_all_embeddings`) or only the embedding for one specific word (`get_word_embedding`). In `cbow.py`, write these methods."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test: `get_all_embeddings` and `get_word_embedding`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.random.set_seed(0)\n",
    "test_net = CBOW(input_feats_shape=(3,), C=3, embedding_dim=5)\n",
    "test_net.compile()\n",
    "print('All the embeddings are:')\n",
    "print(test_net.get_all_embeddings().numpy())\n",
    "print('and they should be:')\n",
    "print('''[[ 0.8724  0.2442 -0.2423 -0.5982 -0.7141]\n",
    " [ 0.2715 -0.0081  0.6864  0.3479  0.3462]\n",
    " [-0.4074 -0.25    0.4582 -0.4027 -0.5542]]''')\n",
    "print('The embedding for index 1 is')\n",
    "print(test_net.get_word_embedding(1).numpy())\n",
    "print('and it should be:')\n",
    "print('''[ 0.2715 -0.0081  0.6864  0.3479  0.3462]''')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run/adapt the following cell to save your embeddings to disk (`net` is your net trained on the reviews)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "net.save_embeddings()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 3: Visualizing word embeddings with t-SNE\n",
    "\n",
    "In this task, you will use the **cosine similarity** metric to find the words that have the most similar embedding to some query words of your choice. You will use the [t-SNE dimensionality reduction algorithm ](https://en.wikipedia.org/wiki/T-distributed_stochastic_neighbor_embedding) built into scikit-learn to visualize in 2D the relative positioning of the query word and words with the highest cosine similarity."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3a. Implement cosine similarity to get the most similar words to a query word\n",
    "\n",
    "Given a word that we are interested in, the cosine similarity will find the $k$ words that have the most similar embeddings to that of the query word.\n",
    "\n",
    "Here is a refresher on the cosine similarity equation:\n",
    "\n",
    "$$\n",
    "\\text{Cosine Similarity} = \\frac{(\\text{Wts})(\\vec{w})}{\\sqrt{\\sum_{j=1}^H (\\text{Wts}^T)_j^2} \\sqrt{\\sum_{j=1}^H w_j^2}}\n",
    "$$\n",
    "\n",
    "Where $\\text{Wts}$ are all the embeddings, $\\text{Wts}^T$ is the transpose of the embeddings, $\\vec{w}$ is the word embedding vector for the query word.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from amazon_reviews import get_most_similar_words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test: `get_most_similar_words`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_word_str2int = {\n",
    "                     'Waterville': 0,\n",
    "                     'Acadia': 1,\n",
    "                     'Camden': 2,\n",
    "                     'Portland': 3,\n",
    "                     'Boothbay': 4,\n",
    "                     'Bangor': 5,\n",
    "                     'Kennebunkport': 6\n",
    "                    }\n",
    "test_word_int2str = {\n",
    "                     0: 'Waterville',\n",
    "                     1: 'Acadia',\n",
    "                     2: 'Camden',\n",
    "                     3: 'Portland',\n",
    "                     4: 'Boothbay',\n",
    "                     5: 'Bangor',\n",
    "                     6: 'Kennebunkport'\n",
    "                    }\n",
    "\n",
    "test_word_str = 'Waterville'\n",
    "tf.random.set_seed(0)\n",
    "test_embeddings = tf.random.uniform(shape=(7, 4)).numpy()\n",
    "test_top_inds, test_top_sims = get_most_similar_words(k=3,\n",
    "                                                      all_embeddings=test_embeddings,\n",
    "                                                      word_str=test_word_str,\n",
    "                                                      word_str2int=test_word_str2int)\n",
    "\n",
    "print(f'Words most similar to {test_word_str}:')\n",
    "for k0 in range(len(test_top_inds)):\n",
    "    print(f'{k0}: {test_word_int2str[test_top_inds[k0]]} (similarity={test_top_sims[k0]:.4f})')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3b. Use t-SNE to perform dimensionality reduction on the embeddings.\n",
    "\n",
    "In the cell below:\n",
    "1. Use [scikit-learn's TSNE class](https://scikit-learn.org/stable/modules/generated/sklearn.manifold.TSNE.html) to reduce the dimensionality of the learned Amazon word embeddings down to 2D so that we will be able to visualize them in a scatter plot.\n",
    "2. Assign the original embeddings to a variable `embeddings`.\n",
    "3. Assign the 2D embeddings to a variable `word_tnse`. Make sure `embeddings` is a NumPy ndarray rather than a TF tensor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.manifold import TSNE\n",
    "from amazon_reviews import find_unique_word_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3c. Create word cloud from Amazon Fashion review embeddings\n",
    "\n",
    "In the cell below:\n",
    "\n",
    "1. Name the word string-to-int map `word2ind` and the word index int-to-string map `ind2word` for the Amazon Fashion dataset.\n",
    "2. Name the vocabulary for the Amazon Fashion dataset `vocab`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pick a word for `query_word` (has it be in the vocab) and run the cell below to show the words with the 25 most similar embeddings as well as a scatter plot, which shows all the word embeddings but annotates the query word and the most similar words.\n",
    "\n",
    "**Note:** At least to start, pick query words among those that appear the most in the corpus. Code two cells down lists these most frequent words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_word = 'great'\n",
    "topk_word_inds, topk_cossim = get_most_similar_words(k=25,\n",
    "                                                     all_embeddings=embeddings,\n",
    "                                                     word_str=query_word,\n",
    "                                                     word_str2int=word2ind)\n",
    "\n",
    "\n",
    "word_strs = [ind2word[ind] for ind in topk_word_inds]\n",
    "\n",
    "print(f'Words most similar to {query_word}:')\n",
    "for k0 in range(len(topk_word_inds)):\n",
    "    print(f'{k0}: {ind2word[topk_word_inds[k0]]} (similarity={topk_cossim[k0]:.4f})')\n",
    "\n",
    "# Visualize word cloud — each word as (x, y) coords\n",
    "plt.figure(figsize=(40, 40))\n",
    "plt.scatter(word_tnse[:, 0], word_tnse[:, 1])\n",
    "\n",
    "max_xy = 5\n",
    "rng = np.random.default_rng(0)\n",
    "offsets = rng.uniform(low=-max_xy, high=max_xy, size=(len(word_strs), 2))\n",
    "for w in range(len(word_strs)):\n",
    "    plt.annotate(word_strs[w], (word_tnse[w, 0]+offsets[w,0], word_tnse[w, 1]+offsets[w,0]), fontsize=30)\n",
    "\n",
    "plt.title('Word Embeddings (2D t-SNE)')\n",
    "plt.xlabel('Embedding dim 1')\n",
    "plt.ylabel('Embedding dim 2')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Print the words that appear most often in the Amazon Fashion corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: modify variable name `corpus` to refer to your Amazon Fashion corpus\n",
    "unique_word_counts = find_unique_word_counts(corpus=corpus)\n",
    "top_k = 100\n",
    "\n",
    "print(f'Top {top_k} words (by count in corpus):')\n",
    "i = 0\n",
    "for word, count in unique_word_counts.items():\n",
    "    print(word, count)\n",
    "    i += 1\n",
    "\n",
    "    if i > top_k:\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3d. Questions\n",
    "\n",
    "**Question 1:** Have some fun looking up word similarities. Which similarities do you like best / find the most interesting?\n",
    "\n",
    "**Question 2:** Do the most similar words tend to show up nearby or far from one another in the word cloud?\n",
    "\n",
    "**Question 3:** Why does the quality of the similar words improve for words that are more frequent in the corpus?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer 1:** \n",
    "\n",
    "**Answer 2:** \n",
    "\n",
    "**Answer 3:** "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extensions\n",
    "\n",
    "### General guidelines\n",
    "\n",
    "1. Never integrate extensions into your base project so that they change the expected behavior of core functions. If your extension changes the core design/behavior, no problem, duplicate your working base project and add features from there.\n",
    "2. Check the rubric to keep in mind how extensions on this project will be graded.\n",
    "3. While I may consult your code and \"written log\" of what you did, **I am grading your extensions based on what you present in your 3-5 min video.**\n",
    "3. I suggest documenting your explorations in a \"log\" or \"lab notebook\" style (i.e. documenting your thought/progression/discovery/learning process). I'm not grading your writing, so you can keep it succinct. **Whatever is most useful to you to remember what you did.** \n",
    "4. I suggest taking a hypothesis driven approach. For example \"I was curious about X so I explored Y. I found Z, which was not what I expected because..., so then tried A...\"\n",
    "5. Make plots to help showcase your results.\n",
    "6. **More is not necessarily better.** Generally, a small number of \"in-depth\" extensions count for more than many \"shallow\" extensions.\n",
    "\n",
    "### AI guidelines\n",
    "\n",
    "You may use AI in mostly any capacity for extensions. However, keep in mind:\n",
    "1. There is no need to use AI at all!\n",
    "2. You are welcome to use AI as a tool (e.g. automate something that is tedious, help you get unstuck, etc.). However, you should be coding, you should be thinking, you should be writing, you should be creating. If you are spending most (or even close to most) of your time typing into a chatbot and copy-pasting, you have probably gone too far with AI use.\n",
    "3. I don't find large volumes of AI generated code/text/plots to be particularly impressive and you risk losing my interest while grading. Remember: I'm grading your extensions based on your video presentation. **More is not necessarily better.**\n",
    "\n",
    "### Video guidelines\n",
    "\n",
    "1. Please try to keep your video to 5 minutes (*I have other projects to grade!*). If you turn in a longer video, I make no promise that I will watch more than 5 minutes.\n",
    "2. Your screen should be shared as you show me what you did. A live video of your face should also appear somewhere on the screen (e.g. picture-in-picture overlay / split screen).\n",
    "3. Your partner should join you for the video and take turns talking, but, if necessary, it is fine to have one team member present during the record the video.\n",
    "4. Do not simply read text from your notebook, do not read from a prepared script. I am not grading how polished your video presentation is (see extension grading criteria on rubric). \n",
    "5. I am looking for original and creative explorations sparked by your curiosity/interest/passion in a topic. This should be apparent in your video.\n",
    "6. Be natural,, don't feel the need to impress me with fancy language. If it is helpful, imagine that we are talking one-on-one about your extension. Tell me what you did :)\n",
    "\n",
    "### Extension ideas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. Analyze effect of embedding dimension\n",
    "\n",
    "- Systematically vary the embedding dimension. How does the embedding dimension affect the quality of similar words based on their embeddings / cosine similarity?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Amount of data\n",
    "\n",
    "How does the size of the dataset (number of reviews) affect the quality of the embeddings?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. Improve text preprocessing\n",
    "\n",
    "Try implementing and seeing how any of the following may change the quality of embeddings:\n",
    "\n",
    "**Stemming:** Currently words with different suffixes are treated the same — e.g. \"run\", \"runs\", \"running\", etc. Normalize these so they map the same word.\n",
    "\n",
    "**Remove stop words:** Remove common \"filler\" words that have little meaning — e.g. \"a\", \"the\", \"an\", etc.\n",
    "\n",
    "**Misspelled words**: There are numerous misspellings of words in the corpus. Does having them in the vocab help or hurt?\n",
    "\n",
    "If things improve/worsen, why might this be the case?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4. Other text dataset of your choice\n",
    "\n",
    "Obtain and preprocess a text dataset of your choice. Then either train on CBOW and visualize/analyze embeddings."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5. Sentiment analysis\n",
    "\n",
    "(*This is a more involved/challenging extension*) Use the word embedding vectors as data samples into another neural network whose job it is to predict whether either the word or the Amazon review to which it and other words in the review belong is positive (say >3 star rating) or negative (say < 3 star rating). \n",
    "\n",
    "For predicting the sentiment of words, you can use a simple heuristic to get the +/- label for individual words: If the word belongs to more + reviews than - reviews, then the word is a + word (and vice versa)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6. Other dimensionality reduction techniques\n",
    "\n",
    "Explore how different dimensionality reduction techniques alter the visualization/clustering of the words. *Keep in mind that many algorithms have hyperparameters and if one algorithm gives poor results, you should explore other hyperparameter values before concluding the algorithm does a poor job.*\n",
    "\n",
    "Some popular ideas:\n",
    "- PCA\n",
    "- UMAP\n",
    "- Self-organizing map (SOM)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cs444",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
